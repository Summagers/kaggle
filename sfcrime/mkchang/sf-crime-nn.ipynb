{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: San Francisco Crime Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the category of crimes that occurred in the city by the bay\n",
    "\n",
    "From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz.\n",
    "\n",
    "Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay.\n",
    "\n",
    "From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given time and location, you must predict the category of crime that occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import datasets, cross_validation, metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import csv\n",
    "from copy import copy\n",
    "\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_features(data, gps_data):\n",
    "    # Convert Police district to one-hot matrix\n",
    "    data = pd.concat([data, pd.get_dummies(data['PdDistrict'])], axis=1)\n",
    "    \n",
    "    # Extract date-time features\n",
    "    data['Dates'] = pd.to_datetime(data['Dates'])\n",
    "    data['year'] = data['Dates'].dt.year\n",
    "    data['month'] = data['Dates'].dt.month\n",
    "    data['day'] = data['Dates'].dt.day\n",
    "    data['hour'] = data['Dates'].dt.hour\n",
    "    data['minute'] = data['Dates'].dt.minute\n",
    "    data['dayofyear'] = data['Dates'].dt.dayofyear\n",
    "    data['dayofweek'] = data['Dates'].dt.dayofweek\n",
    "\n",
    "    data['Z'] = gps_data['altitude (ft)']\n",
    "    data[['X','Y','Z']] = preprocessing.normalize(data[['X','Y','Z']], norm='l2')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def make_PCA(X, n_comp):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(X)\n",
    "    return pca\n",
    "\n",
    "def build_model(input_dim, output_dim, hn=32, dp=0.5, layers=1,\n",
    "                init_mode='glorot_uniform',\n",
    "                batch_norm=True):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hn, input_dim=input_dim, init=init_mode))\n",
    "    model.add(Activation('relu'))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dp))\n",
    "\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(hn, init=init_mode))\n",
    "        model.add(Activation('relu'))\n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dropout(dp))\n",
    "\n",
    "    model.add(Dense(output_dim, init=init_mode))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model_weights(model, name):\n",
    "    try:\n",
    "        model.save_weights(name, overwrite=True)\n",
    "    except:\n",
    "        print(\"failed to save classifier weights\")\n",
    "    pass\n",
    "\n",
    "def load_model_weights(model, name):\n",
    "    try:\n",
    "        model.load_weights(name)\n",
    "    except:\n",
    "        print(\"Can't load weights!\")\n",
    "\n",
    "\n",
    "def run_model(X, y, model,batch_size, nb_epoch, lr, load_name='SF-crime.h5', save_name='SF-crime.h5'):\n",
    "    adam = Adam(lr=lr)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam)\n",
    "    load_model_weights(model, load_name)\n",
    "    model.fit(X,\n",
    "              y,\n",
    "              nb_epoch=nb_epoch,\n",
    "              batch_size=batch_size,\n",
    "              validation_split=0.1,\n",
    "              show_accuracy=True,\n",
    "              verbose=True)\n",
    "\n",
    "    save_model_weights(model, save_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_PCA = True\n",
    "save_preds = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set paths for data to be imported\n",
    "\n",
    "home = expanduser('~')\n",
    "# path = str(home) + '\\\\Documents\\\\data-science\\\\kaggle\\\\sf-crime\\\\' # Windows\n",
    "# path = str(home) + '/Documents/Personal/Summagers/kaggle/sfcrime/mkchang/' # Mac\n",
    "path = str(home) + '/Documents/Summagers/kaggle/sfcrime/mkchang/' # Linux\n",
    "trainfile = 'train.csv'\n",
    "testfile = 'test.csv'\n",
    "train_gps_file = 'train_gps.csv'\n",
    "test_gps_file = 'test_gps.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = make_features(pd.read_csv(path+trainfile), pd.read_csv(path+train_gps_file))\n",
    "test_data = make_features(pd.read_csv(path+testfile), pd.read_csv(path+test_gps_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove training data with incorrect latitude and longitude\n",
    "train_data = train_data[train_data['Y']!=90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decide which features to go into training set\n",
    "features = ['dayofyear','dayofweek','hour','X','Y','Z','BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK',\n",
    "       'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all = train_data.ix[:,features]\n",
    "y_all = train_data.ix[:,'Category']\n",
    "X_test = test_data.ix[:,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = y_all.astype('category').cat.codes\n",
    "\n",
    "X = X_all.as_matrix()\n",
    "if use_PCA:\n",
    "    pca = make_PCA(X, 15)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, train_size=.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_OH = np_utils.to_categorical(y.as_matrix(), y.nunique())\n",
    "y_train_OH = np_utils.to_categorical(y_train.as_matrix(), y.nunique())\n",
    "# y_cv_OH = np_utils.to_categorical(y_cv.as_matrix(), y.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "output_dim = y_OH.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model(input_dim, output_dim, hn=256, dp=0.5, layers=5, init_mode='glorot_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't load weights!\n",
      "Train on 790244 samples, validate on 87805 samples\n",
      "Epoch 1/20\n",
      "790244/790244 [==============================] - 143s - loss: 2.6931 - acc: 0.2000 - val_loss: 2.6864 - val_acc: 0.1890\n",
      "Epoch 2/20\n",
      "790244/790244 [==============================] - 145s - loss: 2.6313 - acc: 0.2141 - val_loss: 2.6618 - val_acc: 0.1936\n",
      "Epoch 3/20\n",
      "790244/790244 [==============================] - 137s - loss: 2.6210 - acc: 0.2172 - val_loss: 2.6477 - val_acc: 0.2010\n",
      "Epoch 4/20\n",
      "790244/790244 [==============================] - 141s - loss: 2.6148 - acc: 0.2187 - val_loss: 2.6312 - val_acc: 0.1991\n",
      "Epoch 5/20\n",
      "790244/790244 [==============================] - 146s - loss: 2.6105 - acc: 0.2201 - val_loss: 2.6366 - val_acc: 0.1972\n",
      "Epoch 6/20\n",
      "790244/790244 [==============================] - 147s - loss: 2.6070 - acc: 0.2210 - val_loss: 2.6368 - val_acc: 0.2033\n",
      "Epoch 7/20\n",
      "790244/790244 [==============================] - 145s - loss: 2.6052 - acc: 0.2209 - val_loss: 2.6333 - val_acc: 0.2054\n",
      "Epoch 8/20\n",
      "790244/790244 [==============================] - 147s - loss: 2.6028 - acc: 0.2212 - val_loss: 2.6271 - val_acc: 0.2050\n",
      "Epoch 9/20\n",
      "790244/790244 [==============================] - 151s - loss: 2.6013 - acc: 0.2213 - val_loss: 2.6273 - val_acc: 0.2002\n",
      "Epoch 10/20\n",
      "790244/790244 [==============================] - 154s - loss: 2.5995 - acc: 0.2216 - val_loss: 2.6173 - val_acc: 0.2046\n",
      "Epoch 11/20\n",
      "790244/790244 [==============================] - 169s - loss: 2.5980 - acc: 0.2216 - val_loss: 2.6279 - val_acc: 0.2015\n",
      "Epoch 12/20\n",
      "790244/790244 [==============================] - 186s - loss: 2.5973 - acc: 0.2217 - val_loss: 2.6052 - val_acc: 0.2057\n",
      "Epoch 13/20\n",
      "790244/790244 [==============================] - 187s - loss: 2.5962 - acc: 0.2221 - val_loss: 2.6205 - val_acc: 0.2057\n",
      "Epoch 14/20\n",
      "790244/790244 [==============================] - 192s - loss: 2.5955 - acc: 0.2225 - val_loss: 2.6122 - val_acc: 0.2024\n",
      "Epoch 15/20\n",
      "790244/790244 [==============================] - 187s - loss: 2.5953 - acc: 0.2224 - val_loss: 2.6269 - val_acc: 0.2029\n",
      "Epoch 16/20\n",
      "790244/790244 [==============================] - 189s - loss: 2.5942 - acc: 0.2225 - val_loss: 2.6160 - val_acc: 0.2075\n",
      "Epoch 17/20\n",
      "790244/790244 [==============================] - 191s - loss: 2.5938 - acc: 0.2220 - val_loss: 2.6136 - val_acc: 0.2054\n",
      "Epoch 18/20\n",
      "790244/790244 [==============================] - 190s - loss: 2.5931 - acc: 0.2228 - val_loss: 2.6073 - val_acc: 0.2083\n",
      "Epoch 19/20\n",
      "790244/790244 [==============================] - 189s - loss: 2.5924 - acc: 0.2228 - val_loss: 2.6150 - val_acc: 0.2069\n",
      "Epoch 20/20\n",
      "790244/790244 [==============================] - 192s - loss: 2.5921 - acc: 0.2227 - val_loss: 2.6156 - val_acc: 0.2038\n"
     ]
    }
   ],
   "source": [
    "model = run_model(X, y_OH, model, 256, 20, 1e-2, load_name='SF-crime_FC256x5_PCA-15_train-0.5.h5', save_name='SF-crime_FC256x5_PCA-15_train-0.5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884262/884262 [==============================] - 54s    \n"
     ]
    }
   ],
   "source": [
    "if save_preds:\n",
    "    X_final_test = X_test[features].as_matrix()\n",
    "    X_final_test = pca.transform(X_final_test)\n",
    "    pred = model.predict_proba(X_final_test, batch_size=256, verbose=1)\n",
    "\n",
    "    labels = list(pd.get_dummies(train_data['Category']).columns)\n",
    "\n",
    "    with open('sf-nn.csv', 'w') as outf:\n",
    "        fo = csv.writer(outf, lineterminator='\\n')\n",
    "        fo.writerow(['Id'] + labels)\n",
    "        for i, p in enumerate(pred):\n",
    "            fo.writerow([i] + list(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
