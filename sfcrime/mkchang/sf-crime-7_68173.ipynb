{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: San Francisco Crime Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the category of crimes that occurred in the city by the bay\n",
    "\n",
    "From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz.\n",
    "\n",
    "Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay.\n",
    "\n",
    "From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given time and location, you must predict the category of crime that occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation, preprocessing\n",
    "from os.path import expanduser, normpath\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Gradient Tree Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set paths for data to be imported\n",
    "\n",
    "home = expanduser('~')\n",
    "# path = str(home) + '\\\\Documents\\\\data-science\\\\kaggle\\\\sf-crime\\\\' # Windows\n",
    "path = str(home) + '/Documents/Personal/Summagers/kaggle/sfcrime/mkchang/' # Mac\n",
    "trainfile = 'train.csv'\n",
    "testfile = 'test.csv'\n",
    "train_gps_file = 'train_gps.csv'\n",
    "test_gps_file = 'test_gps.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_raw = pd.read_csv(path+trainfile)\n",
    "test_data_raw = pd.read_csv(path+testfile)\n",
    "train_gps = pd.read_csv(path+train_gps_file)\n",
    "test_gps = pd.read_csv(path+test_gps_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data_raw.copy()\n",
    "test_data = test_data_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary features\n",
    "train_data.drop(['Descript', \n",
    "                 'Resolution', \n",
    "                 'PdDistrict', \n",
    "                 'DayOfWeek', \n",
    "                 'Address'], inplace=True, axis=1)\n",
    "\n",
    "test_data.drop(['PdDistrict', \n",
    "                'DayOfWeek', \n",
    "                'Address'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['Dates'] = pd.to_datetime(train_data['Dates'])\n",
    "train_data['year'] = train_data['Dates'].dt.year\n",
    "train_data['month'] = train_data['Dates'].dt.month \n",
    "train_data['day'] = train_data['Dates'].dt.day\n",
    "train_data['hour'] = train_data['Dates'].dt.hour\n",
    "train_data['minute'] = train_data['Dates'].dt.minute\n",
    "\n",
    "train_data['dayofyear'] = train_data['Dates'].dt.dayofyear\n",
    "train_data['dayofweek'] = train_data['Dates'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['Dates'] = pd.to_datetime(test_data['Dates'])\n",
    "test_data['year'] = test_data['Dates'].dt.year\n",
    "test_data['month'] = test_data['Dates'].dt.month \n",
    "test_data['day'] = test_data['Dates'].dt.day\n",
    "test_data['hour'] = test_data['Dates'].dt.hour\n",
    "test_data['minute'] = test_data['Dates'].dt.minute\n",
    "\n",
    "test_data['dayofyear'] = test_data['Dates'].dt.dayofyear\n",
    "test_data['dayofweek'] = test_data['Dates'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['Z'] = train_gps['altitude (ft)']\n",
    "test_data['Z'] = test_gps['altitude (ft)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove training data with incorrect latitude and longitude\n",
    "train_data = train_data[train_data['Y']!=90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decide which features to go into training set\n",
    "features = ['dayofyear','dayofweek','hour','X','Y','Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train_data.ix[:,features]\n",
    "y_train = train_data.ix[:,'Category']\n",
    "X_test = test_data.ix[:,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate training and cross-validation features\n",
    "X_train, X_cv, y_train, y_cv = cross_validation.train_test_split(X_train, \n",
    "                                                                 y_train, \n",
    "                                                                 test_size=.3, \n",
    "                                                                 random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # polarize data\n",
    "#     if tod:\n",
    "#         times = index.hour\n",
    "#         tody = np.cos(2*np.pi*times/24)\n",
    "#         todx = np.sin(2*np.pi*times/24)     \n",
    "        \n",
    "#         X_train[:,2] = tody[shuffling][:n_points]\n",
    "#         X_train[:,3] = todx[shuffling][:n_points]\n",
    "        \n",
    "#         X_test[:,2] = tody[shuffling][n_points:]\n",
    "#         X_test[:,3] = todx[shuffling][n_points:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crime_forest = RandomForestClassifier(n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 10 s, total: 2min 49s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%time crime_forest = crime_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "score_train = crime_forest.score(X_train, y_train)\n",
    "score_cv = crime_forest.score(X_cv, y_cv)\n",
    "\n",
    "# test/train\n",
    "# 20/80 split Training Score: 0.944199898638 , CV Score: 0.217073344343\n",
    "# 50/50 split Training Score: 0.894782517584 , CV Score: 0.242728773988\n",
    "# 80/20 split Training Score: 0.943824063687 , CV Score: 0.219235806617\n",
    "print ('Training Score:', score_train, ', CV Score:', score_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33612205,  0.11773859,  0.14074625,  0.13833773,  0.1442002 ,\n",
       "        0.12285517])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 trees, no max leaf nodes\n",
    "crime_forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_prediction = crime_forest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(prob_prediction, index=X_test.index, columns=crime_forest.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission_2016_03_19.csv', index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.datasets import load_digits\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "X, y = X_train, y_train\n",
    "train_data = X\n",
    "# estimator = crime_forest\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "estimator = GradientBoostingClassifier()\n",
    "title = \"Learning Curves (Random Forest)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = cross_validation.ShuffleSplit(train_data.shape[0], n_iter=5,\n",
    "                                   test_size=0.3, random_state=0)\n",
    "\n",
    "plot_learning_curve(estimator, title, X, y, n_jobs=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try gradient boosted\n",
    "# change tree sizes\n",
    "# add features (Z above sea level), replace hour with time of day\n",
    "# try regularization (in ensemble) to correct overfitting\n",
    "# voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split again, generate training and cross-validation features for grid search\n",
    "X_grid_train, X_grid_cv, y_grid_train, y_grid_cv = cross_validation.train_test_split(X_train, \n",
    "                                                                                     y_train, \n",
    "                                                                                     test_size=0.40, \n",
    "                                                                                     random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [200], 'min_samples_split': [1, 2]}\n",
    "]\n",
    "scores = ['precision', 'recall']\n",
    "# , 'max_features': [2, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = GridSearchCV(RandomForestClassifier(), param_grid, error_score=0, n_jobs=1)\n",
    "clf.fit(X_grid_train, y_grid_train)\n",
    "\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean_score, scores.std() * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(OneVsRestClassifier(SVC()), param_grid,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    clf.fit(X_grid_train, y_grid_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_grid_cv, clf.predict(X_grid_cv)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
