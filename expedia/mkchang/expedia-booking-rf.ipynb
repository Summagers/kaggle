{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expedia Hotel Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pd.set_option('display.max_columns', 30) # 27 columns of data in training set\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn import datasets, cross_validation, metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_features(data, destinations_data):    \n",
    "    '''\n",
    "    Extract date-time features from dataframe 'data'.\n",
    "    Converts date_time, srch_ci, and srch_co fields  (if they exist) \n",
    "    into components (year, month, day, etc) and drops the original field.\n",
    "    '''\n",
    "    \n",
    "    # Extract date-time features\n",
    "    fields = ['date_time', 'srch_ci', 'srch_co']\n",
    "    for field in fields:\n",
    "        if field in data.keys():\n",
    "            extract_datetimes(data, field)\n",
    "            data = data.drop(field, axis=1)\n",
    "    \n",
    "    \n",
    "    # merge in srch_destination_id d1-d149 fields\n",
    "    data = pd.merge(data, destinations_data, on='srch_destination_id', how='left')\n",
    "\n",
    "#     has_null = ['orig_destination_distance', 'srch_ci_year', 'srch_ci_month', \n",
    "#                 'srch_ci_day', 'srch_ci_hour', 'srch_ci_minute', \n",
    "#                 'srch_ci_dayofyear', 'srch_ci_dayofweek', 'srch_co_year', \n",
    "#                 'srch_co_month', 'srch_co_day', 'srch_co_hour', 'srch_co_minute',\n",
    "#                 'srch_co_dayofyear', 'srch_co_dayofweek']\n",
    "#     has_null = ['orig_destination_distance', 'srch_ci_year', 'srch_ci_hour',\n",
    "#                 'srch_ci_dayofyear', 'srch_ci_dayofweek', 'srch_co_year', \n",
    "#                 'srch_co_hour', 'srch_co_dayofyear', 'srch_co_dayofweek']\n",
    "#     has_null.extend(destinations_data.columns[1:])\n",
    "    \n",
    "    # Only impute columns with nulls\n",
    "    has_null = data.columns[data.isnull().sum()>0]\n",
    "    \n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0, copy=False)\n",
    "    data[has_null] = imp.fit_transform(data[has_null])\n",
    "    return data\n",
    "\n",
    "def extract_datetimes(data, field):\n",
    "    data[field] = pd.to_datetime(data[field],errors='coerce')\n",
    "    data[field+'_year'] = data[field].dt.year\n",
    "#     data[field+'_month'] = data[field].dt.month\n",
    "#     data[field+'_day'] = data[field].dt.day\n",
    "#   data[field+'_hour'] = data[field].dt.hour\n",
    "#     data[field+'_minute'] = data[field].dt.minute\n",
    "    data[field+'_dayofyear'] = data[field].dt.dayofyear\n",
    "    data[field+'_dayofweek'] = data[field].dt.dayofweek\n",
    "\n",
    "def make_PCA(X, n_comp):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(X)\n",
    "    return pca\n",
    "\n",
    "def apk(actual, predicted, k=5):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=5):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "def mapk_score(X, y, estimator, num_splits=10):\n",
    "# Score MAP@5 for X using estimator against target y, \n",
    "# splits into num_splits sets to reduce memory requirement\n",
    "\n",
    "    n_test = X.shape[0]\n",
    "    top_pred_hotel_cluster = []\n",
    "\n",
    "    for i in range(num_splits):\n",
    "        prob_prediction = estimator.predict_proba(X_train[int(i*n_test/num_splits):int((i+1)*n_test/num_splits)])\n",
    "        top_pred_hotel_cluster.extend((np.argsort(prob_prediction)[:,-5:]).tolist())\n",
    "\n",
    "    return mapk([[i] for i in y.values.tolist()], top_pred_hotel_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_PCA = False\n",
    "save_preds = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['site_name', 'posa_continent', 'user_location_country',\n",
    "       'user_location_region', 'user_location_city',\n",
    "       'orig_destination_distance', 'user_id', 'is_mobile', 'is_package',\n",
    "       'channel', 'srch_ci', 'srch_co', 'srch_adults_cnt', 'srch_children_cnt',\n",
    "       'srch_rm_cnt', 'srch_destination_id', 'srch_destination_type_id',\n",
    "       'is_booking', 'cnt', 'hotel_country',\n",
    "       'hotel_cluster']\n",
    "# 'date_time', 'hotel_market', 'hotel_continent',\n",
    "\n",
    "cols_test = cols[:17]\n",
    "cols_test.extend(cols[19:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample_sub = pd.read_csv('data/sample_submission.csv')\n",
    "destinations_data = pd.read_csv('data/destinations.csv')\n",
    "train_data = make_features(pd.read_csv('data/train.csv', usecols=cols, nrows=1000000),\n",
    "                           destinations_data) # 37,670,294 total lines\n",
    "test_data = make_features(pd.read_csv('data/test.csv', usecols=cols_test, nrows=5), \n",
    "                          destinations_data)   # 2,528,244 total lines\n",
    "# test_data = pd.read_csv('data/test_maked.csv') #full csv, pre-made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ('posa_continent', 0.0)\n",
    "# ('user_location_country', 0.0)\n",
    "# ('srch_adults_cnt', 0.0)\n",
    "# ('srch_destination_id', 0.0)\n",
    "# ('srch_destination_type_id', 0.0)\n",
    "# ('hotel_continent', 0.0)\n",
    "# ('hotel_country', 0.0)\n",
    "# ('hotel_market', 0.0)\n",
    "# ('date_time_hour', 0.0)\n",
    "# ('date_time_dayofyear', 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take features from columns in test data, ignoring some fields uniquely in train\n",
    "features = test_data.columns.tolist()[1:]\n",
    "\n",
    "X_all = train_data.ix[:,features]\n",
    "y_all = train_data.ix[:,'hotel_cluster']\n",
    "X_test = test_data.ix[:,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # checking correlation of features\n",
    "# plt.matshow(X_all.corr())\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    X = X_all.as_matrix()\n",
    "    pca = make_PCA(X, 50)\n",
    "    X = pca.transform(X)\n",
    "else:\n",
    "    X = X_all   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate training and cross-validation features\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y_all, train_size=.9, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train: Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expedia_rfc = RandomForestClassifier(n_estimators=1200, \n",
    "                                     max_leaf_nodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time expedia_rfc = expedia_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(expedia_rfc, 'model/expedia_rfc.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "num_splits=500\n",
    "\n",
    "score_train = mapk_score(X_train, y_train, expedia_rfc, num_splits=num_splits)\n",
    "score_cv = mapk_score(X_cv, y_cv, expedia_rfc, num_splits=num_splits)\n",
    "\n",
    "# train/cv\n",
    "print ('Training Score:', score_train, '\\nCV Score:', score_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importance = zip(features, expedia_rfc.feature_importances_)\n",
    "for x in sorted(feature_importance, key=lambda x: -x[1]):\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expedia_rfc = joblib.load('model/expedia_rfc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    X_test = pca.transform(X_test.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# break the test set into n = num_split sets to predict on\n",
    "num_splits = 10\n",
    "n_test = X_test.shape[0]\n",
    "top_pred_hotel_cluster = []\n",
    "\n",
    "for i in range(num_splits):\n",
    "    prob_prediction = expedia_rfc.predict_proba(X_test[int(i*n_test/num_splits):int((i+1)*n_test/num_splits)])\n",
    "    top_pred_hotel_cluster.extend([' '.join([str(hotel) for hotel in row]) for row in np.argsort(prob_prediction)[:,-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(top_pred_hotel_cluster, columns=['hotel_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if submission.shape[0] == 2528243:\n",
    "    submission.to_csv('expedia-rf-2016-05-01-s1.csv', index_label='Id')\n",
    "else:\n",
    "    print('submission size does not match correct value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.datasets import load_digits\n",
    "from sklearn import cross_validation\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "estimator = expedia_lr\n",
    "\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# estimator = GradientBoostingClassifier()\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=5,\n",
    "                                   test_size=0.3, random_state=0)\n",
    "\n",
    "plot_learning_curve(estimator, title, X, y_all, n_jobs=2, cv=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exhaustive Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split again, generate training and cross-validation features for grid search\n",
    "X_grid_train, X_grid_cv, y_grid_train, y_grid_cv = cross_validation.train_test_split(X_train, \n",
    "                                                                                     y_train, \n",
    "                                                                                     test_size=0.40, \n",
    "                                                                                     random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [200], 'min_samples_split': [1, 2]}\n",
    "]\n",
    "scores = ['precision', 'recall']\n",
    "# , 'max_features': [2, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = GridSearchCV(RandomForestClassifier(), param_grid, error_score=0, n_jobs=1)\n",
    "clf.fit(X_grid_train, y_grid_train)\n",
    "\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean_score, scores.std() * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(OneVsRestClassifier(SVC()), param_grid,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    clf.fit(X_grid_train, y_grid_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_grid_cv, clf.predict(X_grid_cv)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
