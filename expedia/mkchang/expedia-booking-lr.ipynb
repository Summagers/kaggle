{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expedia Hotel Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 30) # 27 columns of data in training set\n",
    "\n",
    "# import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn import datasets, cross_validation, metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "# from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_features(data):    \n",
    "    '''\n",
    "    Extract date-time features from dataframe 'data'.\n",
    "    Converts date_time, srch_ci, and srch_co fields into\n",
    "    components (year, month, day, etc) and drops the \n",
    "    original field.\n",
    "    '''\n",
    "    extract_datetimes(data, 'date_time')\n",
    "    extract_datetimes(data, 'srch_ci')\n",
    "    extract_datetimes(data, 'srch_co')\n",
    "    \n",
    "    data = data.drop(['date_time', 'srch_ci', 'srch_co'], axis=1)\n",
    "    \n",
    "    has_null = ['orig_destination_distance', 'srch_ci_year', 'srch_ci_month', \n",
    "                'srch_ci_day', 'srch_ci_hour', 'srch_ci_minute', \n",
    "                'srch_ci_dayofyear', 'srch_ci_dayofweek', 'srch_co_year', \n",
    "                'srch_co_month', 'srch_co_day', 'srch_co_hour', 'srch_co_minute',\n",
    "                'srch_co_dayofyear', 'srch_co_dayofweek']\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0, copy=False)\n",
    "    data[has_null] = imp.fit_transform(data[has_null])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_datetimes(data, field):\n",
    "    data[field] = pd.to_datetime(data[field],errors='coerce')\n",
    "    data[field+'_year'] = data[field].dt.year\n",
    "    data[field+'_month'] = data[field].dt.month\n",
    "    data[field+'_day'] = data[field].dt.day\n",
    "    data[field+'_hour'] = data[field].dt.hour\n",
    "    data[field+'_minute'] = data[field].dt.minute\n",
    "    data[field+'_dayofyear'] = data[field].dt.dayofyear\n",
    "    data[field+'_dayofweek'] = data[field].dt.dayofweek\n",
    "\n",
    "def make_PCA(X, n_comp):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(X)\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_PCA = False\n",
    "save_preds = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('data/sample_submission.csv')\n",
    "train_data = make_features(pd.read_csv('data/train.csv', nrows=10000)) # 37,670,294 total lines\n",
    "test_data = make_features(pd.read_csv('data/test.csv'))   # 2,528,244 total lines\n",
    "destinations_data = pd.read_csv('data/destinations.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_data #['srch_destination_id'].apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# destinations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# destinations_data[destinations_data['srch_destination_id']==8250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d = {'user_id':1, 'user':'user1', 'group_id':3, 'group_name':'ordinary users'}\n",
    "# m = {'user_id':'uid', 'group_id':'gid', 'group_name':'group'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b = dict((m.get(k, k), v) for (k, v) in d.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# m.get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_data['srch_destination_id'].get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# destinations_data.ix[8250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data['srch_destination_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# destinations_data[destinations_data['srch_destination_id']==train_data['srch_destination_id'][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['site_name', 'posa_continent', 'user_location_country',\n",
    "       'user_location_region', 'user_location_city',\n",
    "       'orig_destination_distance', 'user_id', 'is_mobile', 'is_package',\n",
    "       'channel', 'srch_adults_cnt', 'srch_children_cnt', 'srch_rm_cnt',\n",
    "       'srch_destination_id', 'srch_destination_type_id',\n",
    "       'hotel_continent', 'hotel_country', 'hotel_market',\n",
    "       'date_time_year', 'date_time_month', 'date_time_day', 'date_time_hour',\n",
    "       'date_time_minute', 'date_time_dayofyear', 'date_time_dayofweek',\n",
    "       'srch_ci_year', 'srch_ci_month', 'srch_ci_day', 'srch_ci_hour',\n",
    "       'srch_ci_minute', 'srch_ci_dayofyear', 'srch_ci_dayofweek',\n",
    "       'srch_co_year', 'srch_co_month', 'srch_co_day', 'srch_co_hour',\n",
    "       'srch_co_minute', 'srch_co_dayofyear', 'srch_co_dayofweek']\n",
    "\n",
    "# 'srch_ci', 'srch_co', 'orig_destination_distance', 'is_booking', 'cnt',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all = train_data.ix[:,features]\n",
    "y_all = train_data.ix[:,'hotel_cluster']\n",
    "X_test = test_data.ix[:,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # checking correlation of features\n",
    "# plt.matshow(X_all.corr())\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    X = X_all.as_matrix()\n",
    "    pca = make_PCA(X, 15)\n",
    "    X = pca.transform(X)\n",
    "else:\n",
    "    X = X_all    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate training and cross-validation features\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y_all, train_size=.7, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expedia_lr = LogisticRegression(multi_class='ovr', \n",
    "                                solver='newton-cg', \n",
    "                                max_iter=1000, \n",
    "                                penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time expedia_lr = expedia_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "score_train = expedia_lr.score(X_train, y_train)\n",
    "score_cv = expedia_lr.score(X_cv, y_cv)\n",
    "\n",
    "# train/cv\n",
    "print ('Training Score:', score_train, ', CV Score:', score_cv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    X_test = pca.transform(X_test.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# break the test set into n = num_split sets to predict on\n",
    "# split_size = 0\n",
    "num_splits = 10\n",
    "n_test = X_test.shape[0]\n",
    "top_pred_hotel_cluster = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(num_splits):\n",
    "    prob_prediction = expedia_lr.predict_proba(X_test[int(i*n_test/num_splits):int((i+1)*n_test/num_splits)])\n",
    "    top_pred_hotel_cluster.append([' '.join([str(hotel) for hotel in row]) for row in np.argsort(prob_prediction)[:,-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "# prob_prediction = expedia_lr.predict_proba(X_test[:split_size])\n",
    "# top_pred_hotel_cluster.append([' '.join([str(hotel) for hotel in row]) for row in np.argsort(prob_prediction)[:,-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array(top_pred_hotel_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.concatenate((a[0],a[1],a[2],a[3],a[4],a[5],a[6],a[7],a[8],a[9]), axis=0), columns=['hotel_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('expedia-rf-2016-04-30-s1.csv', index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.datasets import load_digits\n",
    "from sklearn import cross_validation\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "estimator = expedia_lr\n",
    "\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# estimator = GradientBoostingClassifier()\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=5,\n",
    "                                   test_size=0.3, random_state=0)\n",
    "\n",
    "plot_learning_curve(estimator, title, X, y_all, n_jobs=2, cv=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exhaustive Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split again, generate training and cross-validation features for grid search\n",
    "X_grid_train, X_grid_cv, y_grid_train, y_grid_cv = cross_validation.train_test_split(X_train, \n",
    "                                                                                     y_train, \n",
    "                                                                                     test_size=0.40, \n",
    "                                                                                     random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [200], 'min_samples_split': [1, 2]}\n",
    "]\n",
    "scores = ['precision', 'recall']\n",
    "# , 'max_features': [2, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = GridSearchCV(RandomForestClassifier(), param_grid, error_score=0, n_jobs=1)\n",
    "clf.fit(X_grid_train, y_grid_train)\n",
    "\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean_score, scores.std() * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(OneVsRestClassifier(SVC()), param_grid,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    clf.fit(X_grid_train, y_grid_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_grid_cv, clf.predict(X_grid_cv)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
